---
title: "A closer look at the training dynamics of knowledge distillation"
collection: publications
permalink: /publication/2023-03-26-training-dynamics-distillation
excerpt: 'Explored a novel perspective of knowledge distillation through the training dynamics of the projector weights. We proposed a very simple distillation pipeline to attain a new state-of-the-art for the data efficient training of transformer models.'
date: 2023-03-26
venue: 'arXiv preprint'
paperurl: 'https://arxiv.org/abs/2303.11098'
citation: 'Miles, R., & Mikolajczyk, K. (2023). A closer look at the training dynamics of knowledge distillation. arXiv.'
---