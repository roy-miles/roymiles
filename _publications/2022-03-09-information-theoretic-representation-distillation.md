---
title: "Information Theoretic Representation Distillation"
collection: publications
permalink: /publication/2022-03-09-information-theoretic-representation-distillation
excerpt: 'In this work, we proposed an information-theoretic setting for representation distillation. 
Using this framework, we introduce two novel distillation losses that are
very simple and computationally inexpensive to adopt into most deep learning pipelines. We have shown the superiority of our approach compared to
methods of similar computational costs on standard classification benchmarks.
Furthermore, we have shown the applicability of our method to binary networks,
whereby we begin to bridge the performance gap between full-precision and
binary networks.'
date: 2022-03-09
venue: 'arXiv'
paperurl: 'https://arxiv.org/abs/2112.00459'
citation: 'Miles, R., Rodr√≠guez, A. L., & Mikolajczyk, K. (2021). Information Theoretic Representation Distillation.'
figure: '/images/itrd.svg'
---